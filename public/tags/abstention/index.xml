<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Abstention | Bingbing Wen</title>
    <link>http://localhost:64079/tags/abstention/</link>
      <atom:link href="http://localhost:64079/tags/abstention/index.xml" rel="self" type="application/rss+xml" />
    <description>Abstention</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 01 Jul 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:64079/media/icon_hu_982c5d63a71b2961.png</url>
      <title>Abstention</title>
      <link>http://localhost:64079/tags/abstention/</link>
    </image>
    
    <item>
      <title>MARVEL: Modular Abstention for Reliable and Versatile Expert LLMs</title>
      <link>http://localhost:64079/publications/marvel-modular-abstention/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:64079/publications/marvel-modular-abstention/</guid>
      <description>&lt;p&gt;We present MARVEL, a modular abstention framework for building reliable and versatile expert large language models. Our approach enables models to selectively abstain from answering questions they are uncertain about, improving reliability and trustworthiness in AI systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Know Your Limits: A Survey of Abstention in Large Language Models</title>
      <link>http://localhost:64079/publications/abstention-survey/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:64079/publications/abstention-survey/</guid>
      <description>&lt;p&gt;We provide a comprehensive survey of abstention mechanisms in large language models, covering theoretical foundations, practical implementations, and evaluation methodologies. This work serves as a foundation for understanding how LLMs can know their limits and abstain from answering questions they are uncertain about.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Characterizing LLM Abstention Behavior in Science QA with Context Perturbations</title>
      <link>http://localhost:64079/publications/abstention-scienceqa/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:64079/publications/abstention-scienceqa/</guid>
      <description>&lt;p&gt;We characterize how large language models abstain from answering science questions when presented with context perturbations, providing insights into model uncertainty and reliability in scientific reasoning tasks.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

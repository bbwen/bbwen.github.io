<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications | Bingbing Wen</title>
    <link>http://localhost:1313/publications/</link>
      <atom:link href="http://localhost:1313/publications/index.xml" rel="self" type="application/rss+xml" />
    <description>Publications</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 20 Jan 2026 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu_fb558a5ed99f547e.png</url>
      <title>Publications</title>
      <link>http://localhost:1313/publications/</link>
    </image>
    
    <item>
      <title>Clarify or Answer: Reinforcement Learning for Agentic VQA with Context Under-specification</title>
      <link>http://localhost:1313/publications/clarify-or-answer-agentic-vqa/</link>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/clarify-or-answer-agentic-vqa/</guid>
      <description>&lt;p&gt;We introduce a reinforcement learning framework for agentic VQA that explicitly models whether an agent should ask for clarification or answer directly when faced with underspecified visual and textual context.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SusBench: An Online Benchmark for Evaluating Dark Pattern Susceptibility of Computer-Use Agents</title>
      <link>http://localhost:1313/publications/susbench-dark-pattern-agents/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/susbench-dark-pattern-agents/</guid>
      <description>&lt;p&gt;SusBench is an online benchmark designed to measure how vulnerable computer-use agents are to dark patterns embedded in graphical user interfaces, providing a basis for evaluating and improving the safety of interactive agent systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Asking the Missing Piece: Context-Driven Clarification for Ambiguous VQA</title>
      <link>http://localhost:1313/publications/asking-missing-piece-clarification-vqa/</link>
      <pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/asking-missing-piece-clarification-vqa/</guid>
      <description>&lt;p&gt;We explore how VQA systems can ask targeted clarification questions when initial context is ambiguous, improving reliability and interpretability in multimodal reasoning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?</title>
      <link>http://localhost:1313/publications/spuriverse-lvlm-spurious-correlations/</link>
      <pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/spuriverse-lvlm-spurious-correlations/</guid>
      <description>&lt;p&gt;We introduce a benchmark to evaluate whether large vision-language models can move beyond spurious correlations and generalize robustly to distribution shifts that break shortcut cues.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tensorized Clustered LoRA Merging for Multi-Task Interference</title>
      <link>http://localhost:1313/publications/tensorized-clustered-lora-merging/</link>
      <pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/tensorized-clustered-lora-merging/</guid>
      <description>&lt;p&gt;We introduce tensorized clustered LoRA merging to address multi-task interference when large language models are adapted to many tasks via low-rank adapters, improving performance and parameter efficiency.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MARVEL: Modular Abstention for Reliable and Versatile Expert LLMs</title>
      <link>http://localhost:1313/publications/marvel-modular-abstention/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/marvel-modular-abstention/</guid>
      <description>&lt;p&gt;We present MARVEL, a modular abstention framework for building reliable and versatile expert large language models. Our approach enables models to selectively abstain from answering questions they are uncertain about, improving reliability and trustworthiness in AI systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MMMG: A Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation</title>
      <link>http://localhost:1313/publications/mmmg-multitask-multimodal-generation/</link>
      <pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/mmmg-multitask-multimodal-generation/</guid>
      <description>&lt;p&gt;MMMG is an evaluation suite for multitask multimodal generation, enabling more reliable assessment of large multimodal models across a wide range of generation tasks and modalities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AutoScale-Automatic Prediction of Compute-optimal Data Composition for Training LLMs</title>
      <link>http://localhost:1313/publications/autoscale-data-mixing/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/autoscale-data-mixing/</guid>
      <description>&lt;p&gt;We present AutoScale, a method for automatically predicting compute-optimal data composition for training large language models. Our approach improves training efficiency and model performance by optimizing the data mixing strategy during pretraining.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs</title>
      <link>http://localhost:1313/publications/confidence-calibration/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/confidence-calibration/</guid>
      <description>&lt;p&gt;We investigate whether language models mirror human confidence patterns and explore psychological insights to address overconfidence in large language models. Our work provides new perspectives on understanding and improving confidence calibration in AI systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Know Your Limits: A Survey of Abstention in Large Language Models</title>
      <link>http://localhost:1313/publications/abstention-survey/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/abstention-survey/</guid>
      <description>&lt;p&gt;We provide a comprehensive survey of abstention mechanisms in large language models, covering theoretical foundations, practical implementations, and evaluation methodologies. This work serves as a foundation for understanding how LLMs can know their limits and abstain from answering questions they are uncertain about.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mitigating Overconfidence in Large Language Models: A Behavioral Lens on Confidence Estimation and Calibration</title>
      <link>http://localhost:1313/publications/mitigating-overconfidence-llms/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/mitigating-overconfidence-llms/</guid>
      <description>&lt;p&gt;We examine overconfidence in large language models through a behavioral lens and propose approaches to improve calibration so that stated confidence better reflects actual reliability.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Characterizing LLM Abstention Behavior in Science QA with Context Perturbations</title>
      <link>http://localhost:1313/publications/abstention-scienceqa/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/abstention-scienceqa/</guid>
      <description>&lt;p&gt;We characterize how large language models abstain from answering science questions when presented with context perturbations, providing insights into model uncertainty and reliability in scientific reasoning tasks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Laboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT Even in Low-Resource Settings</title>
      <link>http://localhost:1313/publications/laboratory-scale-ai-open-weight-models/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/laboratory-scale-ai-open-weight-models/</guid>
      <description>&lt;p&gt;We study how open-weight models can be deployed in low-resource settings, finding that with appropriate selection and configuration they can rival proprietary systems such as ChatGPT for many practical tasks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OmniMotionGPT: Animal Motion Generation with Limited Data</title>
      <link>http://localhost:1313/publications/omnimotiongpt-animal-motion-generation/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/omnimotiongpt-animal-motion-generation/</guid>
      <description>&lt;p&gt;We introduce OmniMotionGPT, a model for animal motion generation that leverages powerful sequence modeling to perform well even when training data is scarce.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>InfoVisDial: An Informative Visual Dialogue Dataset by Bridging Large Multimodal and Language Models</title>
      <link>http://localhost:1313/publications/infovisdial/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/infovisdial/</guid>
      <description>&lt;p&gt;We present InfoVisDial, a comprehensive visual dialogue dataset created by bridging large multimodal and language models to enable informative conversations about visual content. This work was conducted during an internship at Microsoft Azure AI.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CCQ: cross-class query network for partially labeled organ segmentation</title>
      <link>http://localhost:1313/publications/ccq-cross-class-query/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/ccq-cross-class-query/</guid>
      <description>&lt;p&gt;We propose CCQ, a cross-class query network for partially labeled organ segmentation, addressing the challenge of learning from incomplete annotations in medical image analysis. This work was presented at AAAI 2023.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EGCR: Explanation Generation for Conversational Recommendation</title>
      <link>http://localhost:1313/publications/egcr-conversational-recommendation-explanations/</link>
      <pubDate>Sat, 20 Aug 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/egcr-conversational-recommendation-explanations/</guid>
      <description>&lt;p&gt;We propose EGCR, a framework for generating explanations in conversational recommendation scenarios that supports multi-turn dialogue and user-centric justification of recommendations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards Generating Robust, Fair, and Emotion-Aware Explanations for Recommender Systems</title>
      <link>http://localhost:1313/publications/robust-fair-emotion-aware-explanations/</link>
      <pubDate>Sat, 20 Aug 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/robust-fair-emotion-aware-explanations/</guid>
      <description>&lt;p&gt;We explore methods for generating explanations in recommender systems that remain robust under perturbations, treat users fairly, and take emotional impact into account.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ExpScore: Learning metrics for recommendation explanation</title>
      <link>http://localhost:1313/publications/expscore-recommendation/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/expscore-recommendation/</guid>
      <description>&lt;p&gt;We propose ExpScore, a framework for learning metrics to evaluate recommendation explanations, improving the quality and interpretability of recommendation systems. This work was presented at WWW 2022.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

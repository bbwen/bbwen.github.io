<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>1 | Bingbing Wen</title>
    <link>http://localhost:1313/publication_types/1/</link>
      <atom:link href="http://localhost:1313/publication_types/1/index.xml" rel="self" type="application/rss+xml" />
    <description>1</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 01 Jan 2026 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu_fb558a5ed99f547e.png</url>
      <title>1</title>
      <link>http://localhost:1313/publication_types/1/</link>
    </image>
    
    <item>
      <title>SusBench: An Online Benchmark for Evaluating Dark Pattern Susceptibility of Computer-Use Agents</title>
      <link>http://localhost:1313/publications/susbench-dark-pattern-agents/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/susbench-dark-pattern-agents/</guid>
      <description>&lt;p&gt;SusBench is an online benchmark designed to measure how vulnerable computer-use agents are to dark patterns embedded in graphical user interfaces, providing a basis for evaluating and improving the safety of interactive agent systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Asking the Missing Piece: Context-Driven Clarification for Ambiguous VQA</title>
      <link>http://localhost:1313/publications/asking-missing-piece-clarification-vqa/</link>
      <pubDate>Mon, 01 Dec 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/asking-missing-piece-clarification-vqa/</guid>
      <description>&lt;p&gt;We explore how VQA systems can ask targeted clarification questions when initial context is ambiguous, improving reliability and interpretability in multimodal reasoning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?</title>
      <link>http://localhost:1313/publications/spuriverse-lvlm-spurious-correlations/</link>
      <pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/spuriverse-lvlm-spurious-correlations/</guid>
      <description>&lt;p&gt;We introduce a benchmark to evaluate whether large vision-language models can move beyond spurious correlations and generalize robustly to distribution shifts that break shortcut cues.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MARVEL: Modular Abstention for Reliable and Versatile Expert LLMs</title>
      <link>http://localhost:1313/publications/marvel-modular-abstention/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/marvel-modular-abstention/</guid>
      <description>&lt;p&gt;We present MARVEL, a modular abstention framework for building reliable and versatile expert large language models. Our approach enables models to selectively abstain from answering questions they are uncertain about, improving reliability and trustworthiness in AI systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AutoScale-Automatic Prediction of Compute-optimal Data Composition for Training LLMs</title>
      <link>http://localhost:1313/publications/autoscale-data-mixing/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/autoscale-data-mixing/</guid>
      <description>&lt;p&gt;We present AutoScale, a method for automatically predicting compute-optimal data composition for training large language models. Our approach improves training efficiency and model performance by optimizing the data mixing strategy during pretraining.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do Language Models Mirror Human Confidence? Exploring Psychological Insights to Address Overconfidence in LLMs</title>
      <link>http://localhost:1313/publications/confidence-calibration/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/confidence-calibration/</guid>
      <description>&lt;p&gt;We investigate whether language models mirror human confidence patterns and explore psychological insights to address overconfidence in large language models. Our work provides new perspectives on understanding and improving confidence calibration in AI systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mitigating Overconfidence in Large Language Models: A Behavioral Lens on Confidence Estimation and Calibration</title>
      <link>http://localhost:1313/publications/mitigating-overconfidence-llms/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/mitigating-overconfidence-llms/</guid>
      <description>&lt;p&gt;We examine overconfidence in large language models through a behavioral lens and propose approaches to improve calibration so that stated confidence better reflects actual reliability.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Characterizing LLM Abstention Behavior in Science QA with Context Perturbations</title>
      <link>http://localhost:1313/publications/abstention-scienceqa/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/abstention-scienceqa/</guid>
      <description>&lt;p&gt;We characterize how large language models abstain from answering science questions when presented with context perturbations, providing insights into model uncertainty and reliability in scientific reasoning tasks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Laboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT Even in Low-Resource Settings</title>
      <link>http://localhost:1313/publications/laboratory-scale-ai-open-weight-models/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/laboratory-scale-ai-open-weight-models/</guid>
      <description>&lt;p&gt;We study how open-weight models can be deployed in low-resource settings, finding that with appropriate selection and configuration they can rival proprietary systems such as ChatGPT for many practical tasks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OmniMotionGPT: Animal Motion Generation with Limited Data</title>
      <link>http://localhost:1313/publications/omnimotiongpt-animal-motion-generation/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/omnimotiongpt-animal-motion-generation/</guid>
      <description>&lt;p&gt;We introduce OmniMotionGPT, a model for animal motion generation that leverages powerful sequence modeling to perform well even when training data is scarce.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CCQ: cross-class query network for partially labeled organ segmentation</title>
      <link>http://localhost:1313/publications/ccq-cross-class-query/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/ccq-cross-class-query/</guid>
      <description>&lt;p&gt;We propose CCQ, a cross-class query network for partially labeled organ segmentation, addressing the challenge of learning from incomplete annotations in medical image analysis. This work was presented at AAAI 2023.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ExpScore: Learning metrics for recommendation explanation</title>
      <link>http://localhost:1313/publications/expscore-recommendation/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/expscore-recommendation/</guid>
      <description>&lt;p&gt;We propose ExpScore, a framework for learning metrics to evaluate recommendation explanations, improving the quality and interpretability of recommendation systems. This work was presented at WWW 2022.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
